# QLora
1.LoRA微调原理
LoRA(LoRA Attention)是一种轻量级的模型调优技术,可以在保持模型大小不变的情况下,对大型预训练语言模型进行高效微调。LoRA微调的原理可概括为以下几个方面:

1. **参数分解**

LoRA将预训练模型的注意力权重矩阵W进行分解,将其表示为两个较小的投影矩阵相乘:

W = W_base + BA

其中W_base是预训练模型原始的注意力权重矩阵,B和A是需要学习的小矩阵,它们的大小远小于W。

2. **微调注意力层**

在微调过程中,LoRA只更新B和A这两个小矩阵,而原始预训练模型的参数W_base保持不变。这种参数分解技术大大减少了需要微调的参数量。

3. **合并注意力层**

在推理时,LoRA将学习到的BA与原始W_base合并,重构出完整的注意力权重矩阵W,以便对输入进行注意力计算。

4. **计算高效**

由于只需要存储和计算投影矩阵B和A,LoRA可以在不增加模型大小和显存占用的情况下,完成对大型语言模型的微调。这使得LoRA非常适合在资源受限的环境下进行微调。

5. **层次控制**

LoRA允许在模型的不同层次(如每个Transformer层或多头注意力头)上应用不同的投影矩阵,从而实现对模型各部分的精细控制。

LoRA微调技术的优势在于:

- 高效利用参数,减小内存需求
- 保持模型大小不变,推理效率高
- 支持多任务和指令遵循
- 可以组合多个LoRA权重进行组合微调

因此,LoRA被广泛应用于大型语言模型的微调,如GPT、LLaMA等,用于各种自然语言处理任务,如问答、摘要、文本生成等。通过LoRA微调,可以在不增加过多计算开销的情况下,有效提升语言模型在特定任务上的性能表现。

2.LoRA微调的主要参数有那几个？
LoRA微调涉及到以下几个主要参数:

1. **Rank(r)**
   - 这是LoRA投影矩阵B和A的秩(rank)。
   - 秩越大,模型就能捕获更多的知识,但同时也需要更多参数。
   - 通常秩设置为较小的值(如4、8、16),以在参数量和模型性能之间取得平衡。

2. **Alpha(α)** 
   - LoRA中的缩放因子,控制微调后的权重偏离原始权重的程度。
   - 较大的α值可以加速收敛,但可能导致模型性能下降。
   - 较小的α值则需要更多训练步数,但可以获得更好的性能。

3. **Dropout rate**
   - 应用于投影矩阵B和A的dropout率,用于防止过拟合。
   - 通常设置为较小的值,如0.1。

4. **训练批量大小(Batch Size)和学习率(Learning Rate)**
   - 这两个是常见的训练超参数。
   - 适当的批量大小和学习率有助于模型更好地收敛。

5. **微调层级(LoRA层)**
   - LoRA允许在不同层级(如每个Transformer层或多头注意力头)上应用不同的投影矩阵。
   - 选择合适的微调层级对最终性能有很大影响。

此外,还有一些辅助参数,如权重衰减系数、梯度裁剪阈值等,用于控制训练过程的稳定性。

这些参数的选择需要根据具体任务、模型大小和可用资源进行调优,以达到最优的模型性能和效率权衡。合理设置LoRA参数对于充分发挥其优势至关重要。
